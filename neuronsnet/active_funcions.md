Softmax究竟是什么意思   应用到多分类问题

假如说我有两个数，a和b，并且a&gt;b，如果取max，那么就直接取a，没有第二种可能.但有的时候我不想这样，因为这样会造成分值小的那个饥饿。所以我希望分值大的那一项经常取到，分值小的那一项也偶尔可以取到.
那么我用softmax就可以了现在还是a和b，a&gt;b，如果我们取按照softmax来计算取a和b的概率，那a的softmax值大于b的，所以a会经常取到，而b也会偶尔取到，概率跟它们本来的大小有关。所以说不是max，而是 <b>Soft</b> max

假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的Softmax值就是 是该元素的指数，与所有元素指数和的比值

 <p data-pid="mKn43rg0"> 
那各自的概率究竟是多少呢，我们下面就来具体看一下</p><h2>定义</h2><p data-pid="nnSyA8G7">假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的Softmax值就是 </p><img src="https://www.zhihu.com/equation?tex=S_i+%3D+%5Cfrac%7Be%5E%7BV_i%7D%7D%7B%5Csum_j%7Be%5E%7BV_j%7D%7D%7D" alt="[公式]" eeimg="1" data-formula="S_i = \frac{e^{V_i}}{\sum_j{e^{V_j}}}"><p data-pid="JIMj5ir6">也就是说，是该元素的指数，与所有元素指数和的比值</p><p data-pid="ervhx7bz">这个定义可以说非常的直观，当然除了直观朴素好理解以外，它还有更多的优点</p><h2>1.计算与标注样本的差距</h2><p data-pid="NMty9a6Y">在神经网络的计算当中，我们经常需要计算按照神经网络的正向传播计算的分数S1，和按照正确标注计算的分数S2，之间的差距，计算Loss，才能应用反向传播。<b>Loss定义为交叉熵</b></p><br><img src="https://www.zhihu.com/equation?tex=L_i%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j%7Be%5Ej%7D%7D%29" alt="[公式]" eeimg="1" data-formula="L_i=-log(\frac{e^{f_{y_i}}}{\sum_j{e^j}})"><p data-pid="QXcEIdfm">取log里面的值就是这组数据正确分类的Softmax值，它占的比重越大，这个样本的Loss也就越小，这种定义符合我们的要求</p><h2>2.计算上非常非常的方便</h2><p data-pid="OG3MvbED">当我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度</p><p data-pid="UZrwKnDl">我们定义选到yi的概率是</p><img src="https://www.zhihu.com/equation?tex=P_%7By_i%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j%7Be%5Ej%7D%7D" alt="[公式]" eeimg="1" data-formula="P_{y_i}=\frac{e^{f_{y_i}}}{\sum_j{e^j}}"><p data-pid="_auOdB9s">然后我们求Loss对每个权重矩阵的偏导，应用<span><a data-za-not-track-link="true" href="https://www.zhihu.com/search?q=%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A139826397%7D" target="_blank" class="css-1occaib">链式法则<svg width="8px" height="8px" viewBox="0 0 15 15" class="css-ukqak1"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span><i>（中间推导省略）</i>。</p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_i%7D%7D%7B%5Cpartial%7Bf_%7By_i%7D%7D%7D%3D%5Cfrac%7B%5Cpartial%28-%5Cln%28%5Cfrac%7Be%5E%7Bf_%7By_%7Bi%7D%7D%7D%7D%7B%5Csum_%7Bj%7De%5E%7B%7Bj%7D%7D%7D%29%29%7D%7B%5Cpartial%7Bf_%7By_i%7D%7D%7D%3DP_%7Bf_%7By_i%7D%7D-1" alt="[公式]" eeimg="1" data-formula="\frac{\partial{L_i}}{\partial{f_{y_i}}}=\frac{\partial(-\ln(\frac{e^{f_{y_{i}}}}{\sum_{j}e^{{j}}}))}{\partial{f_{y_i}}}=P_{f_{y_i}}-1"><p data-pid="Gi1pp06g"><b>最后结果的形式非常的简单，只要将算出来的概率的向量对应的真正结果的那一维减1，就可以了</b></p><p data-pid="KC3Q7gLS">举个例子，通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 1, 5, 3 ], 
那么概率分别就是[0.015,0.866,0.117],如果这个样本正确的分类是第二个的话，那么计算出来的偏导就是[0.015,0.866−1,0.117]=[0.015,−0.134,0.117]，是不是很简单！！然后再根据这个进行<span><a data-za-not-track-link="true" href="https://www.zhihu.com/search?q=back+propagation&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A139826397%7D" target="_blank" class="css-1occaib">back propagation<svg width="8px" height="8px" viewBox="0 0 15 15" class="css-ukqak1"><path d="M10.89 9.477l3.06 3.059a1 1 0 0 1-1.414 1.414l-3.06-3.06a6 6 0 1 1 1.414-1.414zM6 10a4 4 0 1 0 0-8 4 4 0 0 0 0 8z" fill="currentColor"></path></svg></a></span>就可以了</p></span>